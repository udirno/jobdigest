---
phase: 03-job-fetching-scheduling
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/scheduler.js
  - src/adaptive-distribution.js
  - src/job-fetcher.js
autonomous: true

must_haves:
  truths:
    - "Scheduler calculates next fetch time in user's local timezone and creates chrome.alarm"
    - "Adaptive distribution allocates remaining 50 jobs proportionally to API quality"
    - "Job fetcher orchestrates the full bootstrap flow: 25 Adzuna + 25 JSearch + allocate remaining 50"
    - "Fetch pipeline uses checkpoint-based stages that survive service worker restarts"
    - "Daily cap of 100 jobs is enforced across the entire fetch pipeline"
  artifacts:
    - path: "src/scheduler.js"
      provides: "Alarm scheduling with timezone support"
      exports: ["scheduleDailyFetch", "verifyAlarmExists", "getNextFetchTime"]
    - path: "src/adaptive-distribution.js"
      provides: "Quality-based API allocation logic"
      exports: ["calculateAdaptiveAllocation", "updateAdaptiveMetrics"]
    - path: "src/job-fetcher.js"
      provides: "Main fetch orchestrator with checkpoint recovery"
      exports: ["runJobFetch", "resumeJobFetch"]
  key_links:
    - from: "src/job-fetcher.js"
      to: "src/api/adzuna.js"
      via: "import fetchAdzunaJobs"
      pattern: "import.*fetchAdzunaJobs"
    - from: "src/job-fetcher.js"
      to: "src/api/jsearch.js"
      via: "import fetchJSearchJobs"
      pattern: "import.*fetchJSearchJobs"
    - from: "src/job-fetcher.js"
      to: "src/adaptive-distribution.js"
      via: "import calculateAdaptiveAllocation"
      pattern: "import.*calculateAdaptiveAllocation"
    - from: "src/job-fetcher.js"
      to: "src/keep-alive.js"
      via: "import keepAlive for long operation"
      pattern: "import.*keepAlive"
    - from: "src/job-fetcher.js"
      to: "src/storage.js"
      via: "batch progress checkpointing"
      pattern: "storage\\.setBatchProgress"
    - from: "src/scheduler.js"
      to: "src/storage.js"
      via: "read user settings for fetch time"
      pattern: "storage\\.getSettings"
---

<objective>
Build the fetch pipeline: scheduler for daily alarm timing, adaptive distribution for API allocation logic, and the job-fetcher orchestrator that coordinates the bootstrap fetch flow with checkpoint-based recovery.

Purpose: This is the core fetch engine. The scheduler determines WHEN to fetch. The adaptive distribution determines HOW MANY from each API. The job-fetcher orchestrates the entire flow with service worker resilience. Together they implement the user's locked "bootstrap approach" (25+25, then allocate 50 based on quality).

Output: Three new modules (`src/scheduler.js`, `src/adaptive-distribution.js`, `src/job-fetcher.js`) that together form the complete fetch pipeline, ready to be wired into background.js in Plan 03.
</objective>

<execution_context>
@/Users/udirno/.claude/get-shit-done/workflows/execute-plan.md
@/Users/udirno/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-job-fetching-scheduling/03-RESEARCH.md
@src/storage.js
@src/keep-alive.js
@src/errors.js
@.planning/phases/03-job-fetching-scheduling/03-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create scheduler and adaptive distribution modules</name>
  <files>src/scheduler.js, src/adaptive-distribution.js</files>
  <action>
**src/scheduler.js:**

Import `{ storage }` from `'./storage.js'`.

Export three functions:

1. `async function scheduleDailyFetch(hour = null, minute = null)`:
   - Load settings: `const settings = await storage.getSettings()`
   - Use `hour ?? settings.fetchHour` and `minute ?? settings.fetchMinute`
   - Calculate next occurrence of preferred time in local timezone:
     ```
     const now = new Date();
     const nextFetch = new Date();
     nextFetch.setHours(hour, minute, 0, 0);
     if (nextFetch <= now) {
       nextFetch.setDate(nextFetch.getDate() + 1);
     }
     ```
   - Clear existing alarm first: `await chrome.alarms.clear('daily-job-fetch')`
   - Create alarm: `chrome.alarms.create('daily-job-fetch', { when: nextFetch.getTime(), periodInMinutes: 1440 })`
   - Log: `console.log('Daily fetch scheduled for ${nextFetch.toLocaleString()}')`
   - Return `nextFetch` for caller's use

2. `async function verifyAlarmExists()`:
   - `const alarm = await chrome.alarms.get('daily-job-fetch')`
   - If alarm exists, return `{ exists: true, scheduledTime: new Date(alarm.scheduledTime) }`
   - If missing, log warning, call `scheduleDailyFetch()` to recreate, return `{ exists: false, recreated: true, scheduledTime: nextFetch }`

3. `async function getNextFetchTime()`:
   - `const alarm = await chrome.alarms.get('daily-job-fetch')`
   - If alarm, return `new Date(alarm.scheduledTime)`
   - Otherwise return null

**src/adaptive-distribution.js:**

Import `{ storage }` from `'./storage.js'`.

Export two functions:

1. `async function calculateAdaptiveAllocation(bootstrapResults = null)`:
   - `bootstrapResults` is `{ adzuna: jobArray, jsearch: jobArray }` where each job may have a `score` property
   - If bootstrapResults is null or both arrays are empty, return default `{ adzuna: 25, jsearch: 25 }` (even split)
   - Check if any jobs have scores (Phase 4 will provide them). If no scores present on any job, return default `{ adzuna: 25, jsearch: 25 }` with log: `'No scores available yet, using even split'`
   - If scores ARE present (Phase 4 integrated):
     - Calculate per-API metrics:
       - `avgScore` = mean of scores for that API's jobs
       - `highValueCount` = count of jobs with score >= 80
     - Hybrid quality metric (Claude's discretion choice): `quality = (avgScore * 0.7) + ((highValueCount / bootstrapCount) * 100 * 0.3)`
     - Allocate remaining 50 proportionally: `adzunaAlloc = Math.round((adzunaQuality / totalQuality) * 50)`, jsearch gets the rest
     - Clamp each allocation to minimum 10 (never starve an API completely): if either < 10, set to 10 and give other 40
   - Return `{ adzuna: adzunaAlloc, jsearch: jsearchAlloc }`

2. `async function updateAdaptiveMetrics(adzunaJobs, jsearchJobs)`:
   - Load existing metrics: `const metrics = await storage.getAdaptiveMetrics()`
   - Calculate today's entry for each API:
     ```
     const today = new Date().toISOString().split('T')[0];
     const entry = {
       date: today,
       avgScore: jobs with scores? calculate average : null,
       highValueCount: jobs with scores? count >= 80 : 0,
       jobCount: jobs.length
     };
     ```
   - Push entry to recentWindow, trim to last 7 entries (rolling 7-day window per Claude's discretion choice)
   - Update `lastCalibration` to current ISO timestamp
   - Save: `await storage.setAdaptiveMetrics(metrics)`

Follow existing code style: JSDoc comments on all exported functions, console.log for important operations.
  </action>
  <verify>
Read both files and confirm:
1. scheduler.js exports scheduleDailyFetch, verifyAlarmExists, getNextFetchTime
2. scheduleDailyFetch uses chrome.alarms.create with `when` + `periodInMinutes: 1440`
3. verifyAlarmExists recreates alarm if missing
4. adaptive-distribution.js exports calculateAdaptiveAllocation, updateAdaptiveMetrics
5. calculateAdaptiveAllocation returns default 25/25 when no scores available (stub for Phase 4)
6. Hybrid quality metric uses 70% avgScore + 30% high-value percentage
7. Minimum allocation is 10 per API (never starve)
8. updateAdaptiveMetrics trims recentWindow to 7 entries (rolling window)
  </verify>
  <done>Scheduler creates timezone-aware daily alarms; adaptive distribution calculates proportional API allocation with 25/25 default until Phase 4 provides scores</done>
</task>

<task type="auto">
  <name>Task 2: Create job-fetcher orchestrator with checkpoint recovery</name>
  <files>src/job-fetcher.js</files>
  <action>
Import:
- `{ storage }` from `'./storage.js'`
- `{ keepAlive }` from `'./keep-alive.js'`
- `{ fetchAdzunaJobs }` from `'./api/adzuna.js'`
- `{ fetchJSearchJobs }` from `'./api/jsearch.js'`
- `{ calculateAdaptiveAllocation, updateAdaptiveMetrics }` from `'./adaptive-distribution.js'`

Export two functions:

1. `async function runJobFetch(options = {})`:
   Main entry point for a complete fetch cycle. `options`: `{ manual: boolean }` (true if user triggered manually).

   The pipeline has 4 checkpoint stages: 'bootstrap-adzuna', 'bootstrap-jsearch', 'adaptive-allocation', 'remaining-fetch'.

   Steps:
   a. Check daily cap: `const dailyStats = await storage.getDailyStats()`
      - If `dailyStats.jobsFetched >= 100`, log and return `{ status: 'cap_reached', jobsFetched: 0, message: 'Daily cap of 100 jobs reached' }`
      - Calculate `remaining = 100 - dailyStats.jobsFetched`
      - Adjust bootstrap counts if remaining < 50: each API gets `Math.floor(remaining / 2)`

   b. Create fetch history entry:
      ```
      const historyEntry = {
        date: new Date().toISOString().split('T')[0],
        startedAt: new Date().toISOString(),
        completedAt: null,
        status: 'in_progress',
        jobsFetched: 0,
        adzunaCount: 0,
        jsearchCount: 0,
        errors: [],
        manual: options.manual || false
      };
      ```

   c. Set batch progress: `await storage.setBatchProgress({ inProgress: true, stage: 'bootstrap-adzuna', lastBatchIndex: 0, totalBatches: 4, fetchedJobs: {} })`

   d. Wrap entire operation in `keepAlive.withKeepAlive('job-fetch', async () => { ... })`

   e. Inside keep-alive wrapper, call `await executeFetchPipeline(remaining, historyEntry)`

   f. On success: update historyEntry status to 'success', set completedAt, add to fetch history, clear batch progress, return `{ status: 'success', jobsFetched: historyEntry.jobsFetched, adzunaCount: historyEntry.adzunaCount, jsearchCount: historyEntry.jsearchCount }`

   g. On error: update historyEntry status to 'failed' or 'partial' (if some jobs saved), add error message, save history entry, clear batch progress, return `{ status: 'failed', error: error.message, jobsFetched: historyEntry.jobsFetched }`

2. `async function resumeJobFetch()`:
   Called on service worker restart when batch progress indicates in-progress fetch.

   Steps:
   a. `const progress = await storage.getBatchProgress()`
   b. If `!progress.inProgress`, return null (nothing to resume)
   c. Log: `console.log('Resuming fetch from stage:', progress.stage)`
   d. Create a new historyEntry for the resumed session
   e. Wrap in keepAlive.withKeepAlive and call executeFetchPipeline, starting from `progress.stage`
   f. Same success/error handling as runJobFetch

**Internal function `async function executeFetchPipeline(maxJobs, historyEntry, startStage = 'bootstrap-adzuna')`:**

Uses a fall-through switch (no breaks) for checkpoint stages:

```javascript
const bootstrapCount = Math.min(25, Math.floor(maxJobs / 2));
let adzunaJobs = [];
let jsearchJobs = [];
const allErrors = [];

// Load previously fetched jobs if resuming
const progress = await storage.getBatchProgress();
const savedJobs = progress.fetchedJobs || {};

switch (startStage) {
  case 'bootstrap-adzuna': {
    await storage.setBatchProgress({ inProgress: true, stage: 'bootstrap-adzuna', totalBatches: 4, fetchedJobs: savedJobs });
    try {
      adzunaJobs = await fetchAdzunaJobs(bootstrapCount);
      // Deduplicate against existing stored jobs
      const existingJobs = await storage.getJobs();
      adzunaJobs = adzunaJobs.filter(j => !existingJobs[j.jobId]);
      // Save immediately (checkpoint)
      const adzunaMap = {};
      adzunaJobs.forEach(j => { adzunaMap[j.jobId] = j; });
      await storage.saveJobs(adzunaMap);
      savedJobs.adzunaBootstrap = adzunaJobs.length;
      historyEntry.adzunaCount += adzunaJobs.length;
    } catch (error) {
      console.error('Adzuna bootstrap failed:', error);
      allErrors.push(`Adzuna: ${error.message}`);
      // Continue to JSearch even if Adzuna fails
    }
  }
  // falls through

  case 'bootstrap-jsearch': {
    await storage.setBatchProgress({ inProgress: true, stage: 'bootstrap-jsearch', totalBatches: 4, fetchedJobs: savedJobs });
    try {
      jsearchJobs = await fetchJSearchJobs(bootstrapCount);
      const existingJobs = await storage.getJobs();
      jsearchJobs = jsearchJobs.filter(j => !existingJobs[j.jobId]);
      const jsearchMap = {};
      jsearchJobs.forEach(j => { jsearchMap[j.jobId] = j; });
      await storage.saveJobs(jsearchMap);
      savedJobs.jsearchBootstrap = jsearchJobs.length;
      historyEntry.jsearchCount += jsearchJobs.length;
    } catch (error) {
      console.error('JSearch bootstrap failed:', error);
      allErrors.push(`JSearch: ${error.message}`);
    }
  }
  // falls through

  case 'adaptive-allocation': {
    await storage.setBatchProgress({ inProgress: true, stage: 'adaptive-allocation', totalBatches: 4, fetchedJobs: savedJobs });
    // Calculate how many more jobs to fetch from each API
    const totalBootstrap = (savedJobs.adzunaBootstrap || 0) + (savedJobs.jsearchBootstrap || 0);
    const remainingToFetch = Math.min(maxJobs - totalBootstrap, 50);

    if (remainingToFetch <= 0) {
      // Already at or near cap
      break;
    }

    const allocation = await calculateAdaptiveAllocation({ adzuna: adzunaJobs, jsearch: jsearchJobs });
    // Scale allocation to fit remaining
    const totalAlloc = allocation.adzuna + allocation.jsearch;
    savedJobs.adzunaRemaining = Math.round((allocation.adzuna / totalAlloc) * remainingToFetch);
    savedJobs.jsearchRemaining = remainingToFetch - savedJobs.adzunaRemaining;
  }
  // falls through

  case 'remaining-fetch': {
    await storage.setBatchProgress({ inProgress: true, stage: 'remaining-fetch', totalBatches: 4, fetchedJobs: savedJobs });
    const adzunaRemaining = savedJobs.adzunaRemaining || 25;
    const jsearchRemaining = savedJobs.jsearchRemaining || 25;

    // Fetch remaining from Adzuna
    if (adzunaRemaining > 0) {
      try {
        let moreAdzuna = await fetchAdzunaJobs(adzunaRemaining, { page: 2 });
        const existingJobs = await storage.getJobs();
        moreAdzuna = moreAdzuna.filter(j => !existingJobs[j.jobId]);
        const map = {};
        moreAdzuna.forEach(j => { map[j.jobId] = j; });
        await storage.saveJobs(map);
        historyEntry.adzunaCount += moreAdzuna.length;
      } catch (error) {
        console.error('Adzuna remaining fetch failed:', error);
        allErrors.push(`Adzuna (remaining): ${error.message}`);
      }
    }

    // Fetch remaining from JSearch
    if (jsearchRemaining > 0) {
      try {
        let moreJSearch = await fetchJSearchJobs(jsearchRemaining, { page: 2 });
        const existingJobs = await storage.getJobs();
        moreJSearch = moreJSearch.filter(j => !existingJobs[j.jobId]);
        const map = {};
        moreJSearch.forEach(j => { map[j.jobId] = j; });
        await storage.saveJobs(map);
        historyEntry.jsearchCount += moreJSearch.length;
      } catch (error) {
        console.error('JSearch remaining fetch failed:', error);
        allErrors.push(`JSearch (remaining): ${error.message}`);
      }
    }
    break;
  }
}

// Update daily stats
const totalFetched = historyEntry.adzunaCount + historyEntry.jsearchCount;
historyEntry.jobsFetched = totalFetched;
historyEntry.errors = allErrors;
await storage.incrementDailyCount(totalFetched);

// Update adaptive metrics
await updateAdaptiveMetrics(adzunaJobs, jsearchJobs);
```

Important implementation notes:
- The existing `batchProgress` schema in storage.js uses `{ inProgress, lastBatchIndex, totalBatches }`. Extend the object stored with additional fields (`stage`, `fetchedJobs`) â€” the storage helper just does `set/get`, so this is compatible.
- When resuming, load `progress.fetchedJobs` to know what was already fetched in previous stages.
- Log all major steps: stage transitions, job counts, errors.
  </action>
  <verify>
Read the file and confirm:
1. runJobFetch checks daily cap before starting
2. runJobFetch wraps operation in keepAlive.withKeepAlive
3. executeFetchPipeline uses fall-through switch with 4 stages
4. Each stage saves batch progress before starting (checkpoint)
5. Jobs are deduplicated against existing stored jobs
6. Jobs are saved immediately after each API call (not batched at end)
7. resumeJobFetch loads batch progress and continues from last stage
8. Both API failures are caught individually (one API failing doesn't block the other)
9. Daily count is incremented after fetch completes
10. History entry and adaptive metrics are updated
  </verify>
  <done>Job-fetcher orchestrates the full bootstrap fetch flow (25+25+allocate 50) with 4-stage checkpoint recovery, deduplication, keep-alive, and daily cap enforcement</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. `src/scheduler.js` creates timezone-aware chrome.alarms and verifies alarm persistence
2. `src/adaptive-distribution.js` returns 25/25 default when no scores, proportional allocation when scores available
3. `src/job-fetcher.js` runs complete 4-stage pipeline: bootstrap-adzuna -> bootstrap-jsearch -> adaptive-allocation -> remaining-fetch
4. All three modules import correctly from Plan 01 outputs (storage.js, api/adzuna.js, api/jsearch.js, keep-alive.js, errors.js)
5. Checkpoint recovery works: if service worker dies at stage 2, resumeJobFetch picks up from stage 2
6. Daily cap is checked at start and incremented at end
7. Both APIs can fail independently without killing the entire pipeline
</verification>

<success_criteria>
- Scheduler creates daily alarms at user's preferred time in their local timezone
- Adaptive distribution defaults to 25/25 until Phase 4 provides scoring (graceful degradation)
- Job fetcher runs the complete bootstrap pipeline with checkpoint-based recovery
- Any individual API failure is handled gracefully (partial results still saved)
- Daily cap of 100 jobs is enforced
</success_criteria>

<output>
After completion, create `.planning/phases/03-job-fetching-scheduling/03-02-SUMMARY.md`
</output>
